---
title: "XOR-Some Prolific Pilot 2"
author: "Polina Tsvilodub"
date: "4/2/2021"
output: github_document
---

In this second pilot on Prolific we gather a more substantial amount of pilot data for the xor-some study on Prolific. The structure of the experiment was as follows:
Participants read instructions, completed three example trials, and then completed 8 main blocks consisting of 4 xor and 4 some items. Each main block had the following structure: Participants read the background story, answered one comprehension question, then answered competence / relevance / prior questions in randomized order; then they read another 3 comprehension questions, after which the critical utterance was added below the background story. They answered the inference strength question, and then competence / relevance questions in randomized order again.

N=118 participants were recruited for this pilot and compensated 2 pounds/participant. 8 items (4 some + 4 xor) were sampled at random for each participant such that each participant saw one item in each condition (relevance X competence X prior = 8 unique conditions). 

```{r libraries, include=FALSE}
library(tidyverse)
library(tidyboot)
library(brms)
library(tidybayes)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = FALSE, message = FALSE)
```

```{r anonymization, include=FALSE}
#d_raw <- read_csv("~/Documents/Research/XOR/raw_data/results_58_xor-some-Prolific-pilot2_N120.csv")
# remove IDs
#d_raw %>% select(-prolific_id) %>% write_csv("../data/pilots/results_58_xor-some-Prolific-pilot2_N120.csv") 
```

## Checks & Exclusions
```{r data, results='hide', include=FALSE}
d <- read_csv("../data/pilots/results_58_xor-some-Prolific-pilot2_N120.csv")
```

Checking if there are any comments indicating technical issues which is not the case:
```{r comments}
d %>% distinct(comments) %>% View()
```

Check native languages. Participants not indicating English as (one of) their native language(s) are be excluded. We also compute some participant demographics.
```{r languages}
d %>% distinct(languages)
cat("Number of partiipants before excluding non-natives: ", d %>% distinct(submission_id) %>% count() %>% pull() ) 
# exclude non-natives if necessary
d_native <- d %>% 
 # filter(("en" | "En" | "bri") %in% languages)
  filter(grepl("[(en)(br)]", languages, ignore.case = T))

cat("Number of partiipants after excluding non-natives: ", d_native %>% distinct(submission_id) %>% count() %>% pull() )

cat("Mean age: ", d_native %>% pull(age) %>% mean(., na.rm = T) )
d_native %>% count(gender) %>% mutate(n = n/88)
```

Next, we check whether all the conditions were used correctly.

```{r counts}
# check xor/some vs. trial type
d_native %>% count(main_type, condition) 
# check xor/some vs. experimental condition
d_native %>% count(main_type, exp_condition)
# count items used
d_native %>% count(title)
```

Check the time participants spent overall on the experiment before cleaning the data:
```{r responseTime}
overall_timeSpent <- d_native %>% mutate(timeSpent = round(timeSpent, 2)) %>% distinct(timeSpent) 

#  summarize(timeCounts = count(timeSpent) / d_native %>% )
ggplot(data = overall_timeSpent, aes(y=timeSpent, alpha = 0.7)) +
  geom_boxplot() +
  ggtitle("Overall time participants took in mins")
```
```{r}
d_main <- d_native %>% select(-age, -botresponse, -comments, -education, -endTime, 
                              -gender, -languages, -optionLeft, -optionRight, -startDate,
                              -startTime, -timeSpent) %>%
  filter(trial_name != "example")
d_exmpl <- d_native %>% select(-age, -botresponse, -comments, -education, -endTime, 
                              -gender, -languages, -optionLeft, -optionRight, -startDate,
                              -startTime, -timeSpent) %>%
  filter(trial_name == "example")
d_critical <- d_main %>% filter(condition == "critical")
```

Plot responses on comprehension questions by type before applying exclusion criteria:
```{r, echo=FALSE}
d_test <- d_main %>% rowwise() %>% filter(condition == "test") %>% 
  mutate(test_condition = substr(test_question, 6, 9),
         test_condition = ifelse(test_condition == "fals", "false", 
                                 ifelse(test_condition == "unce", "uncertain",
                                        test_condition)))
d_test_ci <- d_test %>% group_by(test_condition) %>% 
  tidyboot_mean(column = response) 

d_test %>% 
  ggplot(., aes(x = test_condition, y = response)) +
  geom_point(size = 2, alpha = 0.3, position = position_jitter(width = 0.1)) +
  geom_point(data = d_test_ci, aes(x = test_condition, y = mean), color = "red", 
             size = 4) +
  facet_wrap(~main_type)
```


Next, we exclude participants based on their ratings in the main trials: Participants who gave all responses within the range of 10 and participants who failed more than 0.2 of the comprehension questions are excluded from analysis. 
Participants who failed all example trials are excluded, as well. 
The bot check trial is not considered for exclusions. 
```{r exclusions}
# get participants failing example trials
d_exmpl <- d_exmpl %>% group_by(submission_id) %>% 
  mutate(example_condition = ifelse(grepl("as certainly true", question), "true", 
                                    ifelse(grepl("as certainly false", question), "false",
                                           "uncertain")),
         passed_example_trial = case_when(example_condition == "true" ~ response >= 80,
                                    example_condition == "false" ~ response <= 20,
                                    example_condition == "uncertain" ~ response >= 30),
       #  check if all trials passed
         passed_example = case_when(sum(passed_example_trial) == 0 ~ FALSE,
                                    TRUE ~ TRUE)
         ) %>% filter(passed_example == F)

cat("Subjects failing the example trials: ", d_exmpl %>% distinct(submission_id) %>% pull() %>% length())

# apply exclusion criteria to main trials
# check range of responses per participant
d_main_fail <- d_main %>% group_by(submission_id) %>%
  mutate(passed_main = case_when(max(response) - min(response) <= 10 ~ FALSE,
                                 TRUE ~ TRUE)
         ) %>% filter(passed_main == F)
cat("Subjects providing the same ratings throughout the trials: ", d_main_fail %>% distinct(submission_id) %>% pull() %>% length())

# get participants failing comprehension questions
d_test <- d_test %>%
  group_by(submission_id) %>%
  mutate(passed_filler_trial = case_when(test_condition == "true" ~ response >= 70,
                                   test_condition == "false" ~ response <= 40,
                                   test_condition == "uncertain" ~ response %in% (0:90)),
         mean_comprehension = mean(passed_filler_trial),
         passed_filler = mean_comprehension >= 0.8
         ) %>%
  filter(passed_filler == F)

cat("Subjects failing the comprehension trials: ", d_test %>% distinct(submission_id) %>% pull() %>% length())

# put it all together
d_full_clean <- anti_join(d_main, d_main_fail, by = "submission_id")
d_full_clean <- anti_join(d_full_clean, d_exmpl, d_test, by = "submission_id")
d_full_clean <- anti_join(d_full_clean, d_test, by = "submission_id")

cat("Nr. of participants left after cleaning: ", d_full_clean %>% distinct(submission_id) %>% pull() %>% length())
```

```{r clean}
# get overall mean ratings / subject
d_full_clean %>% group_by(submission_id) %>% summarise(mean_rating = mean(response)) %>% arrange(mean_rating)
```

```{r, echo=FALSE}
d_critical_clean <- d_full_clean %>% filter(condition == "critical")
d_critical_long <- d_critical_clean %>% 
  pivot_longer(c(competence, relevance, prior), 
               names_to = "class_condition", 
               values_to = "prior_class")


d_critical_long <- d_critical_long %>% 
  mutate(w_utterance = ifelse(is.na(critical_question), F, T),
         block = ifelse(block == "comp", "competence", 
                        ifelse(block == "rel", "relevance", ifelse(block == "pri", "prior", block) )))
```

Create more extensive condition labels, including information about whether a rating was produced with or without the utterance given.

``` {r}
# extending 'conditions' labels to include whether the utterance was present or not
d_critical_long <- d_critical_long %>% 
  mutate(block_extended = ifelse(
    !w_utterance, 
    block, 
    ifelse(block %in% c("some", "xor"), "target", str_c(block, "_wUtt", ""))
  ))
```

Average nr of responses per question per vignette:
```{r}
d_critical_long %>% 
  filter(class_condition == block | block == "xor" | block == "some") %>%
  select(-class_condition, -prior_class) %>%
  unique() %>% count(title, block_extended, main_type) %>%
  mutate(n = ifelse(main_type == "xor" & block_extended == "prior", n/2, n)) %>%
  summarize(mean_observations = mean(n))
```

Compute the average deviation from expected responses for each vignette, individually for each dimension (rel / comp / pri):
```{r}
d_critical_long %>% 
  filter(block != "xor" & block != "some") %>%
  filter(block == class_condition) %>%
  mutate(expected_rating = prior_class * 100,
         response_deviation = abs(response - expected_rating),
         block = ifelse(block == "xor" | block == "some", "target", block)
         ) %>%
  group_by(title, block) %>%
  summarize(mean_deviation = mean(response_deviation)) %>%
  arrange(desc(mean_deviation), .by_group = T) %>%
  # get the largest deviations - over 50 
  filter(mean_deviation >= 50) -> d_critical_deviations

#d_critical_deviations %>% write_csv("../data/pilots/pilot2_byItem_rating_deviations.csv")

print(d_critical_deviations)

# count which dimension is the most problematic - the prior
d_critical_deviations %>% group_by(block) %>% count()

# check which items have the most deviations over 50 
d_critical_deviations %>% group_by(title) %>% count() %>% arrange(desc(.$n))
```

Compute the standard deviation for each question for each item as an additional measure of spread in the participants' responses within-item:
```{r}
d_critical_long %>% 
  filter(block != "xor" & block != "some") %>%
  filter(block == class_condition) %>%
  mutate(
         block = ifelse(block == "xor" | block == "some", "target", block)
         ) %>%
  group_by(title, block) %>%
  summarize(item_sd = sd(response)) %>%
  arrange(desc(item_sd), .by_group = T) %>%
  # arbitrary set to 25
  filter(item_sd >= 25) -> d_critical_sd
d_critical_sd

#d_critical_sd %>% write_csv("../data/pilots/pilot2_byItem_rating_SDs.csv")
```

Visually check the spread of by-item ratings among subjects for each question:
```{r, fig.height=20, fig.width=10}

d_critical_long %>% 
  mutate(title = paste(title, exp_condition, sep = "_"),
         block = ifelse(block == "xor" | block == "some", "target", block)) %>%
  filter(block == class_condition | block == "target") %>%
  ggplot(., aes(x = block, y = response, color = block)) +
  geom_point(alpha = 0.7, position = position_jitter(width = 0.1)) +
  ylab("Raw responses to respective questions") +
  facet_wrap(main_type~title, ncol = 4) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("By-item by-question raw ratings")
```
For the following items, participants seem  to *disagree* in their judgments for all of the predictors: Emails from a broken laptop, Gift unwrapping, Martha's cookies, Suzie's fruits, Brad's clothes, Crime, maybe more..

Check by-vignette by-predictor empirical means, to see if the range is somewhat covered:
```{r}
d_critical_long %>% 
  mutate(title = paste(title, exp_condition, sep = "_"),
         block = ifelse(block == "xor" | block == "some", "target", block)) %>%
  filter(block == class_condition | block == "target") %>%
  group_by(title, block) %>%
  summarise(mean_response = mean(response)) %>%
  arrange(mean_response)
```

## Plots
Plot main rel / comp / prior questions by main condition and by prior classification of the item (x-axis), separated into with / without critical utterance (shape, color). The prior questions were only used without the utterance. 
```{r}

d_critical_long %>% 
  filter(block != "xor" & block != "some") %>%
  filter(block == class_condition) %>%
  group_by(main_type, class_condition, w_utterance, prior_class) %>% 
  summarize(mean_response = mean(response)) -> d_critical_summary

d_critical_long %>% 
  filter(block != "xor" & block != "some") %>%
  filter(block == class_condition) %>%
  ggplot(., aes(x = as.factor(prior_class), y = response, shape = w_utterance, color = w_utterance)) +
  geom_point(size = 2, alpha = 0.6, position = position_jitter(width = 0.1)) +
  geom_point(data = d_critical_summary, aes(x = as.factor(prior_class), y = mean_response, shape = w_utterance), 
             color = "red", size = 3) +
  ylab("Responses to respective predictor question") +
  xlab("Anticipated categorization of the items (low / high)") +
  facet_wrap(main_type~class_condition) # get ratings from the respective trials only 
 
```
```{r}
# check whether the ratings in the two prior questions are the same in xor 
d_critical_long %>% 
  filter(block != "xor" & block != "some") %>%
  filter(block == class_condition) %>%
  filter(block == "prior", main_type == "xor") %>%
  mutate(priorQ_nr = rep(c(1,2), 396)) -> d_xor_priors
d_xor_priors %>% group_by(priorQ_nr, prior_class) %>%
  summarise(mean = mean(response)) -> d_xor_priors_summary
d_xor_priors %>%
  ggplot(., aes(x = as.factor(priorQ_nr), y = response )) +
  geom_point(size = 2, alpha = 0.6, position = position_jitter(width = 0.1)) +
  geom_point(data = d_xor_priors_summary, aes(x = as.factor(priorQ_nr), y = mean), color = "red", size = 3) +
  ylab("Responses to prior questions") +
  xlab("First vs Second prior question for high vs low prior conditions") +
  facet_wrap(~as.factor(prior_class)) # get ratings from the respective trials only 
 
```

Plot inference ratings as a function of *anticipated* rating of the explanatory factor, similar to the paper: 
``` {r}
d_critical_long %>% 
  filter(block == "xor" | block == "some") %>%
  ggplot(., aes(x = prior_class, y = response)) +
  geom_point(size = 2, alpha = 0.5) +
  geom_smooth(method="lm") +
  ylab("Inference strength ratings") +
  facet_wrap(block~class_condition) +
  xlab("Anticipated categorization of the items") +
  ggtitle("Inference strength ratings by-predictor")
  
```

```{r}
# make a versatile wide representation of the critical data
d_critical_wide <- d_critical_long %>% 
  select(submission_id, title, main_type, block_extended, response) %>% 
  unique() %>% 
  pivot_wider(
    names_from = block_extended, 
    values_from = response, 
    values_fn = mean # getting means for double prior measurement in "xor"
  ) 
```

Plot inference strength ratings against raw predictor ratings from each participant across items:
```{r}
d_critical_wide %>%
  ggplot(., aes(x = relevance, y = target)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_smooth(method = "lm") +
  ylab("Inference strength ratings") +
  facet_wrap(~main_type, ncol = 1) -> p.rel

d_critical_wide %>%
  ggplot(., aes(x = competence, y = target)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_smooth(method = "lm") +
  ylab("") +
  facet_wrap(~main_type, ncol = 1) -> p.comp

d_critical_wide %>%
  ggplot(., aes(x = prior, y = target)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_smooth(method = "lm") +
  ylab("") +
  facet_wrap(~main_type, ncol = 1) -> p.pri

gridExtra::grid.arrange(p.rel, p.comp, p.pri, ncol = 3) 
```

Plot mean ratings (across all participants) for each vignette (with its respective condition indicated in the facet title, in the order relevance/competence/prior), in each condition. For relevance and competence, the color indicates whether it was presented without the utterance or with. 
```{r, fig.width=10, fig.height=30}
bar.width = 0.8
d_critical_long %>% 
  mutate(title = paste(title, exp_condition, sep = "_")) %>%
  filter(block == class_condition | block == "xor" | block == "some") %>%
  group_by(block_extended, title, w_utterance, main_type) %>%
  summarize(mean_rating = mean(response)) %>% 
  ggplot(., aes(x = block_extended, y = mean_rating, fill = w_utterance)) +
  geom_col(alpha = 0.7, width = bar.width, position = position_dodge(width = bar.width)) +
  #geom_point(size = 2, alpha = 0.5, position = position_jitter(width = 0.1)) +
  ylab("Mean responses to respective questions") +
  xlab("Question type") +
  facet_wrap(main_type~title, ncol = 4) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("By-item by-question mean ratings")
```

Plot by-item trends between predictor ratings and target inference strength ratings (relevance / competence ratings with utterance are omitted):
```{r, fig.height=35, fig.width=15}
# make a versatile wide representation of the critical data
d_critical_predictorsXtarget <- d_critical_wide %>% 
  pivot_longer(
    cols = c(relevance, prior, competence, relevance_wUtt, competence_wUtt),
    names_to = "predictors",
    values_to = "predictor_response"
  ) %>%
  filter(predictors != "relevance_wUtt", predictors != "competence_wUtt") 

d_critical_predictorsXtarget %>%
  ggplot(., aes(x = predictor_response, y = target)) +
  geom_point() +
  facet_wrap(title~predictors, ncol= 6) +
  xlab("Response to predictor questions") +
  ylab("Inference strength rating") +
  ggtitle("By-item Inference Strength Ratings vs. Predictor Ratings (left to right)")

```

Plot inference strength ratings against predictor ratings, by-item, using by-item mean ratings across participants. Error bars indicate the standard deviation of the mean:
```{r, fig.height=10, fig.width=8}
d_critical_predictorsXtarget_summary <- d_critical_predictorsXtarget %>% 
  group_by(title, predictors, main_type) %>%
  summarise(mean_target = mean(target),
            mean_predictor = mean(predictor_response), 
            sd_target = sd(target),
            sd_predictor = sd(predictor_response))  


d_critical_predictorsXtarget_summary %>%
  ggplot(., aes(x = mean_predictor, y = mean_target)) +
  geom_point() +
  geom_errorbar(aes(ymin = mean_target - sd_target, ymax = mean_target + sd_target), alpha = 0.7) +
  geom_errorbarh(aes(xmin = mean_predictor - sd_predictor, xmax = mean_predictor + sd_predictor), alpha = 0.7) +
  geom_text(aes(label=title), size = 2.5, nudge_y = 4) +
  geom_smooth(method = "lm") +
  ylab("Mean inference strength rating") +
  xlab("Mean predictor ratings") + 
  facet_wrap(main_type~predictors)
```


``` {r}

# correlation plot for "some"
GGally::ggpairs(
  filter(d_critical_wide, main_type == "some") %>%  
    select(prior, competence, relevance, target, competence_wUtt, relevance_wUtt) 
)
  
# correlation plot for "or"
GGally::ggpairs(
  filter(d_critical_wide, main_type == "xor") %>%  
    select(prior, competence, relevance, target, competence_wUtt, relevance_wUtt)
  )
```


## Explore z-scoring
Visual exploration of z-scoring each measure (rel / comp / pri / target / comp-wUtterance / rel-wUtterance) per participant, collapsing some and or together. 
```{r}
d_critical_zScored <- d_critical_long %>% group_by(submission_id, block_extended) %>%
  mutate(block_mean = mean(response),
         block_sd = sd(response),
         response_centered = (response - block_mean)/block_sd,
         # catch the cases where sd is 0 
         response_centered = ifelse(is.na(response_centered), 0, response_centered))

d_critical_zScored %>% 
  filter(block != "xor" & block != "some") %>%
  filter(block == class_condition) %>%
  group_by(main_type, class_condition, w_utterance, prior_class) %>% 
  summarize(mean_response = mean(response_centered)) -> d_critical_zScore_summary

d_critical_zScored %>% 
  filter(block != "xor" & block != "some") %>%
  filter(block == class_condition) %>%
  ggplot(., aes(x = as.factor(prior_class), y = response_centered, shape = w_utterance, color = w_utterance)) +
  geom_point(size = 2, alpha = 0.6, position = position_jitter(width = 0.1)) +
  geom_point(data = d_critical_zScore_summary, aes(x = as.factor(prior_class), y = mean_response, shape = w_utterance), 
             color = "red", size = 3) +
  ylab("Responses to respective predictor questions") +
  xlab("Anticipated categorization of the items (low / high)") +
  facet_wrap(main_type~class_condition) + # get ratings from the respective trials only 
  ggtitle("Centered responses to by-predictor questions")
```

Plot z-scored predictor ratings against z-scored target inference ratings
```{r}
d_critical_zScored_wide <- d_critical_zScored %>% 
  select(submission_id, title, main_type, block_extended, response_centered) %>% 
  unique() %>% 
  pivot_wider(
    names_from = block_extended, 
    values_from = response_centered, 
    values_fn = mean # getting means for double prior measurement in "xor"
  ) 
  
d_critical_zScored_wide %>%
  ggplot(., aes(x = relevance, y = target)) +
  geom_point(size = 2, alpha = 0.7) +
  ylab("Inference strength ratings") +
  geom_smooth(method = "lm") +
  facet_wrap(~main_type, ncol = 1) -> p.rel.z

d_critical_zScored_wide %>%
  ggplot(., aes(x = competence, y = target)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_smooth(method = "lm") +
  ylab("") +
  facet_wrap(~main_type, ncol = 1) -> p.comp.z

d_critical_zScored_wide %>%
  ggplot(., aes(x = prior, y = target)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_smooth(method = "lm") +
  ylab("") +
  facet_wrap(~main_type, ncol = 1) -> p.pri.z

gridExtra::grid.arrange(p.rel.z, p.comp.z, p.pri.z, ncol = 3) 
```

Correlation plots of z-scored data:
``` {r}

# correlation plot for "some"
GGally::ggpairs(
  filter(d_critical_zScored_wide, main_type == "some") %>% ungroup() %>%  
    select(prior, competence, relevance, target, competence_wUtt, relevance_wUtt)
)
  
# correlation plot for "or"
GGally::ggpairs(
  filter(d_critical_zScored_wide, main_type == "xor") %>%  ungroup() %>%
    select(prior, competence, relevance, target, competence_wUtt, relevance_wUtt)
  )


```


## Stats

Maximal models on raw ratings:
```{r, results='hide'}
# xor, maximal model with interactions and maximal REs
model_xor <- brm(
  bf(target ~ prior*competence*relevance + 
    (1 + prior + competence + relevance || submission_id) +
    (1 | title),
    decomp = "QR"
  ), 
  data = d_critical_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor)
```

Exploring plots of posterior samples for pairwise parameters shows that there is a visible correlation between prior & competence, and a somewhat weaker correlation between competence & relevance. The interaction estimates also show a correlation with the main effects (as a rough overall judgment).   
```{r}
#shinystan::launch_shinystan(model_xor)
```
Plot correlations of main effects
```{r}
bayesplot::mcmc_pairs(model_xor, pars = c("b_prior", "b_competence", "b_relevance"))
```

```{r, results='hide'}
# some, maximal model with interactions and maximal REs
model_some <- brm(
  target ~ prior*competence*relevance + 
    (1 + prior + competence + relevance || submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some)
```

For "some", the correlation of prior and relevance is higher than for prior and competence; there is also a visible correlation of competence and relevance. 
```{r}
#shinystan::launch_shinystan(model_some)
```
```{r}
bayesplot::mcmc_pairs(model_some, pars = c("b_prior", "b_competence", "b_relevance"))
```

#### Smaller models with raw data to check for collinearity 
Models on raw data with only one predictor: 
```{r, results='hide'}
# xor
model_xor_prior <- brm(
  target ~ prior + 
    (1 + prior | submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  control = list(adapt_delta = 0.99),
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_prior)
```

```{r, results='hide'}
# xor
model_xor_comp <- brm(
  target ~ competence + 
    (1 + competence | submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  control = list(adapt_delta = 0.99),
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_comp)
```

```{r, results='hide'}
# xor
model_xor_rel <- brm(
  target ~ relevance + 
    (1 + relevance | submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  control = list(adapt_delta = 0.99),
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_rel)
```

```{r, results='hide'}
# some
model_some_rel <- brm(
  target ~ relevance + 
    (1 + relevance | submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "some"),
  family = "gaussian",
  control = list(adapt_delta = 0.99),
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_rel)
```

```{r, results='hide'}
# some
model_some_prior <- brm(
  target ~ prior + 
    (1 + prior | submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "some"),
  family = "gaussian",
  control = list(adapt_delta = 0.99),
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_prior)
```

```{r, results='hide'}
# some
model_some_comp <- brm(
  target ~ competence + 
    (1 + competence | submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "some"),
  family = "gaussian",
  control = list(adapt_delta = 0.92),
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_comp)
```

With two predictors:
```{r, results='hide'}
# xor
model_xor_pri_comp <- brm(
  target ~ prior*competence + 
    (1 + prior*competence || submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_pri_comp)
```

```{r}
bayesplot::mcmc_pairs(model_xor_pri_comp, pars = c("b_prior", "b_competence"))
```

```{r, results='hide'}
# xor
model_xor_pri_rel <- brm(
  target ~ prior*relevance + 
    (1 + prior*relevance || submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  control = list(adapt_delta = 0.92),
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_pri_rel)
```

```{r}
bayesplot::mcmc_pairs(model_xor_pri_rel, pars = c("b_prior", "b_relevance"))
```

```{r, results='hide'}
# xor
model_xor_rel_comp <- brm(
  target ~ relevance*competence + 
    (1 + relevance*competence || submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_rel_comp)
```

```{r}
bayesplot::mcmc_pairs(model_xor_rel_comp, pars = c("b_competence", "b_relevance"))
```

```{r, results='hide'}
# some
model_some_pri_comp <- brm(
  target ~ prior*competence + 
    (1 + prior*competence || submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_pri_comp)
```

```{r}
bayesplot::mcmc_pairs(model_some_pri_comp, pars = c("b_prior", "b_competence"))
```

```{r, results='hide'}
# some
model_some_pri_rel <- brm(
  target ~ prior*relevance + 
    (1 + prior*relevance || submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_pri_rel)
```

```{r}
bayesplot::mcmc_pairs(model_some_pri_rel, pars = c("b_prior", "b_relevance"))
```

```{r, results='hide'}
# some
model_some_rel_comp <- brm(
  target ~ relevance*competence + 
    (1 + relevance*competence || submission_id) +
    (1 | title),
  data = d_critical_wide %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_rel_comp)
```

```{r}
bayesplot::mcmc_pairs(model_some_rel_comp, pars = c("b_competence", "b_relevance"))
```

#### Stats on z-scored data
The same stats (full, one predictor, to predictors) on z-scored data:
```{r, results='hide'}
# xor, maximal model with interactions and maximal REs on z-scored data
model_xor_zScored <- brm(
  target ~ prior*competence*relevance + 
    (1 + prior + competence + relevance || submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_zScored)
```

```{r}
bayesplot::mcmc_pairs(model_xor_zScored, pars = c("b_prior", "b_relevance", "b_competence"))
```

```{r, results='hide'}
# some, maximal model with interactions and maximal REs
model_some_zScored <- brm(
  target ~ prior*competence*relevance + 
    (1 + prior + competence + relevance || submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_zScored)
```

```{r}
bayesplot::mcmc_pairs(model_some_zScored, pars = c("b_prior", "b_relevance", "b_competence"))
```

#### Smaller models with z-scored data
```{r, results='hide'}
# xor
model_xor_zScored_prior <- brm(
  target ~ prior + 
    (1 + prior | submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_zScored_prior)
```

```{r, results='hide'}
# xor
model_xor_zScored_comp <- brm(
  target ~ competence + 
    (1 + competence | submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_zScored_comp)
```

```{r, results='hide'}
# xor
model_xor_zScored_rel <- brm(
  target ~ relevance + 
    (1 + relevance | submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_zScored_rel)
```

```{r, results='hide'}
# some
model_some_zScored_rel <- brm(
  target ~ relevance + 
    (1 + relevance | submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_zScored_rel)
```

```{r, results='hide'}
# some
model_some_zScored_prior <- brm(
  target ~ prior + 
    (1 + prior | submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_zScored_prior)
```

```{r, results='hide'}
# some
model_some_zScored_comp <- brm(
  target ~ competence + 
    (1 + competence | submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_zScored_comp)
```

With two predictors:
```{r, results='hide'}
# xor
model_xor_zScored_pri_comp <- brm(
  target ~ prior*competence + 
    (1 + prior*competence || submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_zScored_pri_comp)
```

```{r}
bayesplot::mcmc_pairs(model_xor_zScored_pri_comp, pars = c("b_prior", "b_competence"))
```

```{r, results='hide'}
# xor
model_xor_zScored_pri_rel <- brm(
  target ~ prior*relevance + 
    (1 + prior*relevance || submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_zScored_pri_rel)
```

```{r}
bayesplot::mcmc_pairs(model_xor_zScored_pri_rel, pars = c("b_prior", "b_relevance"))
```

```{r, results='hide'}
# xor
model_xor_zScored_rel_comp <- brm(
  target ~ relevance*competence + 
    (1 + relevance*competence || submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_zScored_rel_comp)
```

```{r}
bayesplot::mcmc_pairs(model_xor_zScored_rel_comp, pars = c("b_relevance", "b_competence"))
```

```{r, results='hide'}
# some
model_some_zScored_pri_comp <- brm(
  target ~ prior*competence + 
    (1 + prior*competence || submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_zScored_pri_comp)
```

```{r}
bayesplot::mcmc_pairs(model_some_zScored_pri_comp, pars = c("b_prior", "b_competence"))
```

```{r, results='hide'}
# some
model_some_zScored_pri_rel <- brm(
  target ~ prior*relevance + 
    (1 + prior*relevance || submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_zScored_pri_rel)
```

```{r}
bayesplot::mcmc_pairs(model_some_zScored_pri_rel, pars = c("b_prior", "b_relevance"))
```

```{r, results='hide'}
# some
model_some_zScored_rel_comp <- brm(
  target ~ relevance*competence + 
    (1 + relevance*competence || submission_id) +
    (1 | title),
  data = d_critical_zScored_wide %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_zScored_rel_comp)
```

```{r}
bayesplot::mcmc_pairs(model_some_zScored_rel_comp, pars = c("b_relevance", "b_competence"))
```

#### Stats with dichotomized predictors 
Maximal model on raw inference ratings, using anticipated binary categorization of items as predictors 
```{r, results='hide'}
d_critical_long %>% 
  select(submission_id, title, main_type, block_extended, response, prior_class, class_condition) %>% 
  unique() %>% 
  pivot_wider(
    names_from = class_condition, 
    values_from = prior_class
  )  %>%
  mutate(relevance = factor(relevance, levels = c(1, 0)),
         competence = factor(competence, levels = c(1, 0)),
         prior = factor(prior, levels = c(1, 0))) %>% 
  filter(block_extended == "target") -> d_critical_wide_cat
# sum code predictors
# low prior: -1, high prior:: 1 
contrasts(d_critical_wide_cat$prior) <- contr.sum(2)
# low comp : -1, high comp : 1
contrasts(d_critical_wide_cat$competence) <- contr.sum(2)
# low rel: -1, high rel: 1
contrasts(d_critical_wide_cat$relevance) <- contr.sum(2)

# xor, maximal model with interactions and maximal REs
model_xor_cat <- brm(
  response ~ prior*competence*relevance + 
    (1 + prior + competence + relevance || submission_id) +
    (1 | title),
  data = d_critical_wide_cat %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_cat)
```
We test the following hypotheses for the categorical predictors models (identical for both or and some):
H1: For higher prior of the stronger alternative being true, the inference strength is lower
H2: For higher speaker comptence, the inference strength is higher
H3: For higher listener relevance, the inference strength is higher 
```{r}
# xor
# test H1
hypothesis(model_xor_cat, "prior1 < 0")
# test H2
hypothesis(model_xor_cat, "competence1 > 0")
# test H3
hypothesis(model_xor_cat, "relevance1 > 0")
```

```{r, results='hide'}
model_some_cat <- brm(
  response ~ prior*competence*relevance + 
    (1 + prior + competence + relevance || submission_id) +
    (1 | title),
  data = d_critical_wide_cat %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_cat)
```

```{r}
# some
# test H1
hypothesis(model_some_cat, "prior1 < 0")
# test H2
hypothesis(model_some_cat, "competence1 > 0")
# test H3
hypothesis(model_some_cat, "relevance1 > 0")
```

Maximal model on z-scored inference ratings, using anticipated binary categorization of items as predictors 
```{r, results='hide'}
d_critical_zScored %>% 
  select(submission_id, title, main_type, block_extended, response_centered, prior_class, class_condition) %>% 
  unique() %>% 
  pivot_wider(
    names_from = class_condition, 
    values_from = prior_class
  )  %>%
  mutate(relevance = factor(relevance, levels = c(1, 0)),
         competence = factor(competence, levels = c(1, 0)),
         prior = factor(prior, levels = c(1, 0))) %>% 
  filter(block_extended == "target") -> d_critical_wide_cat_zScored
# sum code predictors
# low prior: -1, high prior:: 1 
contrasts(d_critical_wide_cat_zScored$prior) <- contr.sum(2)
# low comp : -1, high comp : 1
contrasts(d_critical_wide_cat_zScored$competence) <- contr.sum(2)
# low rel: -1, high rel: 1
contrasts(d_critical_wide_cat_zScored$relevance) <- contr.sum(2)

# xor, maximal model with interactions and maximal REs
model_xor_cat_zScored <- brm(
  response_centered ~ prior*competence*relevance + 
    (1 + prior + competence + relevance || submission_id) +
    (1 | title),
  data = d_critical_wide_cat_zScored %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_xor_cat_zScored)
```

```{r}
# xor
# test H1
hypothesis(model_xor_cat_zScored, "prior1 < 0")
# test H2
hypothesis(model_xor_cat_zScored, "competence1 > 0")
# test H3
hypothesis(model_xor_cat_zScored, "relevance1 > 0")
```

```{r, results='hide'}
model_some_cat_zScored <- brm(
  response_centered ~ prior*competence*relevance + 
    (1 + prior + competence + relevance || submission_id) +
    (1 | title),
  data = d_critical_wide_cat_zScored %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```

```{r}
summary(model_some_cat_zScored)
```

```{r}
# some
# test H1
hypothesis(model_some_cat_zScored, "prior1 < 0")
# test H2
hypothesis(model_some_cat_zScored, "competence1 > 0")
# test H3
hypothesis(model_some_cat_zScored, "relevance1 > 0")
```

#### Single predictor z-scored models with categorical predictors
For xor:
```{r, results="hide"}
model_xor_cat_zScored_pri <- brm(
  response_centered ~ prior + 
    (1 + prior | submission_id) +
    (1 | title),
  data = d_critical_wide_cat_zScored %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```
```{r}
summary(model_xor_cat_zScored_pri)
```

```{r, results="hide"}
model_xor_cat_zScored_comp <- brm(
  response_centered ~ competence + 
    (1 + competence | submission_id) +
    (1 | title),
  data = d_critical_wide_cat_zScored %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```
```{r}
summary(model_xor_cat_zScored_comp)
```

```{r, results="hide"}
model_xor_cat_zScored_rel <- brm(
  response_centered ~ relevance + 
    (1 + relevance | submission_id) +
    (1 | title),
  data = d_critical_wide_cat_zScored %>% filter(main_type == "xor"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```
```{r}
summary(model_xor_cat_zScored_rel)
```

For some:
```{r, results="hide"}
model_some_cat_zScored_pri <- brm(
  response_centered ~ prior + 
    (1 + prior | submission_id) +
    (1 | title),
  data = d_critical_wide_cat_zScored %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```
```{r}
summary(model_some_cat_zScored_pri)
```

```{r, results="hide"}
model_some_cat_zScored_comp <- brm(
  response_centered ~ competence + 
    (1 + competence | submission_id) +
    (1 | title),
  data = d_critical_wide_cat_zScored %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```
```{r}
summary(model_some_cat_zScored_comp)
```

```{r, results="hide"}
model_some_cat_zScored_rel <- brm(
  response_centered ~ relevance + 
    (1 + relevance | submission_id) +
    (1 | title),
  data = d_critical_wide_cat_zScored %>% filter(main_type == "some"),
  family = "gaussian",
  cores = 4,
  iter = 3000
)
```
```{r}
summary(model_some_cat_zScored_rel)
```

