---
title: 'Xor-Some Preregistered Study: Preprocessing'
author: "Polina Tsvilodub"
date: "10/18/2021"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(brms)
library(tidyboot)
library(tidybayes)
```

## Read Data

First, the data is loaded.
```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
d <- read_csv("./../../data/main/results_73_xor-some-Prolific-main_N275_anonym.csv")  

# separate some and xor data!
d_some <- d %>% filter(main_type == "some") 
d_xor  <- d %>% filter(main_type == "xor")
```

## Clean data

Checking if there are any comments indicating technical issues, to be excluded then:
```{r comments}
d %>% distinct(comments) %>% View()
```

Apply language related exclusion criterion:
Check native languages. Participants not indicating English as (one of) their native language(s) are excluded. We also compute some participant demographics.
```{r languages}
d %>% distinct(languages)
cat("Number of participants before excluding non-natives: ", d %>% distinct(submission_id) %>% count() %>% pull() ) 
# exclude non-natives if necessary
d_native <- d %>% 
 # filter(("en" | "En" | "bri") %in% languages)
  filter(grepl("eng", languages, ignore.case = T))

cat(" Number of participants after excluding non-natives: ", d_native %>% distinct(submission_id) %>% count() %>% pull() )

d_native %>% distinct(languages)

cat(" Mean age: ", d_native %>% pull(age) %>% mean(., na.rm = T) )
d_native  %>% count(gender) %>% mutate(n = n/80)
```

Check the overall completion time. Participants who complete the experiment in less than 8 minutes are excluded.
```{r}
d_native_completed <- d_native %>% mutate(timeSpent = round(timeSpent, 2)) %>%
  filter(timeSpent > 8) 

cat(" Number of participants who completed the expt in > 8 minutes: ", d_native_completed %>% distinct(submission_id) %>% count() %>% pull() )
```

```{r}
d_native_completed %>% summarize(median_time = median(timeSpent),
                                 mean_time = mean(timeSpent))
```
```{r}
# check the distribution of completion times
d_native_completed %>% 
  ggplot(., aes(x=timeSpent)) +
  geom_density()
```

Apply attention check based exclusion criterion:

```{r}
d_native_attention <- d_native_completed %>% 
  filter(condition == "attention_check") %>% mutate(
  passed_attention_trial = case_when(
    (expected == 0) & (response == 0) ~ T,
    (expected == 100) & (response == 100) ~ T,
    (expected == 50) & (response %>% between(45, 55)) ~ T,
    TRUE ~ F
  )
)

d_native_attention_summary <- d_native_attention %>% 
  group_by(submission_id) %>%
  summarize(attention_prop = mean(passed_attention_trial)) %>%
  mutate(
    passed_attention_check = attention_prop > 0.75
  ) %>% 
  filter(passed_attention_check == TRUE)

d_native_attention_failed <- d_native_attention %>% 
  group_by(submission_id) %>%
  summarize(attention_prop = mean(passed_attention_trial)) %>%
  mutate(
    passed_attention_check = attention_prop > 0.75
  ) %>% 
  filter(passed_attention_check == FALSE)

d_native_attended <- anti_join(d_native_completed, 
                               d_native_attention_failed, 
                               by = c("submission_id"))

cat(" Number of participants failing attention checks: ", d_native_attention_failed %>% distinct(submission_id) %>% count() %>% pull() )

#d_attention_fail_IDs <- right_join(d_raw, d_native_attention_failed, by = c("submission_id")) %>% select(submission_id, prolific_id) %>% distinct()
#d_attention_fail_IDs %>% write_csv("~/Documents/Research/XOR/raw_data/xor-some_prereg_failed_attention_prolificIDs_final.csv")
```

Data frames preparation:

```{r}
d_main <- d_native_attended %>% select(-age, -comments, -education, -endTime, 
                              -gender, -languages, -optionLeft, -optionRight, -startDate,
                              -startTime, -timeSpent, -expected) %>%
  filter(trial_name != "example", trial_name != "attention_check")
d_exmpl <- d_native_attended %>% select(-age, -comments, -education, -endTime, 
                              -gender, -languages, -optionLeft, -optionRight, -startDate,
                              -startTime, -timeSpent, -expected) %>%
  filter(trial_name == "example")
d_critical <- d_main %>% filter(condition == "critical")

d_test <- d_main %>% rowwise() %>% filter(condition == "test") %>% 
  mutate(test_condition = substr(test_question, 6, 9),
         test_condition = ifelse(test_condition == "fals", "false", 
                                 ifelse(test_condition == "unce", "uncertain",
                                        test_condition)))
```

Investigate if the comprehension questions received expected responses on by-question basis:
```{r}
d_test %>% group_by(ID, test_question) %>% 
  mutate(
    expected = case_when( test_condition == "true" ~ 100,
                          test_condition == "false" ~ 0,
                          TRUE ~ 50)
  ) %>%
  summarise(
    expected = mean(expected),
    response = mean(response),
    difference = abs(expected - response)
  ) %>% 
  arrange(., desc(difference))

# look at the vignette text and the qeustion of the worst performing item
d_test %>% filter(ID == 64, test_question == "test_false1") %>% select(prompt, QUD) %>% View()
```

Exclude participants based on example trials, comprehension question performance, and response patterns.
Participants who gave all responses within the range of 10 and participants who failed more than 0.2 of the comprehension questions are excluded from analysis. 
Participants who failed all example trials are excluded, as well. 

```{r exclusions}
# get participants failing example trials
d_exmpl_fail <- d_exmpl %>% group_by(submission_id) %>% 
  mutate(example_condition = ifelse(grepl("as certainly true", question), "true", 
                                    ifelse(grepl("as certainly false", question), "false",
                                           "uncertain")),
         passed_example_trial = case_when(example_condition == "true" ~ response >= 80,
                                    example_condition == "false" ~ response <= 20,
                                    example_condition == "uncertain" ~ response >= 30),
       #  check if all trials passed
         passed_example = case_when(sum(passed_example_trial) == 0 ~ FALSE,
                                    TRUE ~ TRUE)
         ) %>% filter(passed_example == F)

cat("Subjects failing the example trials: ", d_exmpl_fail %>% distinct(submission_id) %>% pull() %>% length())

# apply exclusion criteria to main trials
# check range of responses per participant
d_main_fail <- d_main %>% group_by(submission_id) %>%
  mutate(passed_main = case_when(max(response) - min(response) <= 10 ~ FALSE,
                                 TRUE ~ TRUE)
         ) %>% filter(passed_main == F)
# now check for critical trials only
d_main_critical_fail <- d_main %>% group_by(submission_id) %>% 
  filter(condition == "critical") %>%
  mutate(passed_main = case_when(max(response) - min(response) <= 10 ~ FALSE,
                                 TRUE ~ TRUE)
         ) %>% filter(passed_main == F)
# now check implicature trials only 
d_main_implicature_fail <- d_main %>% group_by(submission_id) %>% 
  filter((block == "xor") | (block == "some")) %>%
  mutate(passed_main = case_when(max(response) - min(response) <= 10 ~ FALSE,
                                 TRUE ~ TRUE)
         ) %>% filter(passed_main == F)
# check if this failure is the case for certain vignettes more often then for others
d_main_implicature_fail %>% ungroup() %>% group_by(ID, block) %>% count() %>% arrange(., desc(n)) # View()
# --> it seems that the vignettes 28 (xor) and 45 (some) have this issue the most
# check how often the single participants do this
d_main_implicature_fail %>% count() # %>% View()
# --> all of them do this on all of the trials - these are 15 people 
# write out a csv with their IDs for an exploratory exclusion
#d_main_implicature_fail %>% count() %>% write_csv("./../../data/main/submission-ids_implicature_fails.csv")

cat(" Subjects providing the same ratings throughout the trials: ", d_main_fail %>% distinct(submission_id) %>% pull() %>% length())

# get participants failing comprehension questions
# Perform exclusions based on comprehension question EXCLUDING the test_false1 question from the vignette
# with the ID=64 which had an incorrect prior classification.
d_test <- d_test %>%
  group_by(submission_id) %>%
  filter(ID != 64 && test_question != "test_false1") %>%
  mutate(passed_filler_trial = case_when(test_condition == "true" ~ response >= 60,
                                   test_condition == "false" ~ response <= 40,
                                   test_condition == "uncertain" ~ response %in% (10:90)),
         mean_comprehension = mean(passed_filler_trial),
         passed_filler = mean_comprehension >= 0.8
         ) 

d_test_fail <- d_test %>% 
  filter(passed_filler == F)

cat(" Subjects failing the comprehension trials: ", d_test_fail %>% distinct(submission_id) %>% pull() %>% length())
```

Write out data for plotting:
```{r}
d_test %>% write_csv("./../../data/main/results_prereg_raw_final_test.csv")
```

```{r}
# put it all together
d_full_clean <- anti_join(d_main, d_main_fail, by = "submission_id")
d_full_clean <- anti_join(d_full_clean, d_exmpl_fail, d_test, by = "submission_id")
d_full_clean <- anti_join(d_full_clean, d_test_fail, by = "submission_id")

# write out tidy data without the participants providing only 100 ratings on the critical trials
d_clean_exclude_implicatureFails <- anti_join(d_full_clean, d_main_implicature_fail, by = "submission_id")

cat(" Nr. of participants left after cleaning: ", d_full_clean %>% distinct(submission_id) %>% pull() %>% length())

cat(" Nr. of participants left after excluding participants failing implicature manipulations: ", d_clean_exclude_implicatureFails %>% distinct(submission_id) %>% pull() %>% length())

```
[-Exclude the 201st participant for target N=200-]
The exclusion of the last participant isn't necessary anymore, since we only have 200 subjects left upon exclusions due to the initially incorrect native language check.


Write out tidy data:
```{r}
d_full_clean %>% write_csv("./../../data/main/results_prereg_final_tidy.csv")

d_clean_exclude_implicatureFails %>% write_csv("./../../data/main/results_prereg_final_tidy_exclude_implcatureFailures.csv")
```

```{r clean}
# get overall mean ratings / subject across comprehension and critical trials 
d_full_clean %>% group_by(submission_id) %>% summarise(mean_rating = mean(response)) %>% arrange(mean_rating)

# get mean ratings / subject in critical trials 
d_full_clean %>% filter(condition == "critical") %>% group_by(submission_id) %>% summarise(mean_rating = mean(response)) %>% arrange(mean_rating)

```

## Transformations

```{r, echo=FALSE}
d_critical_clean <- d_full_clean %>% filter(condition == "critical")
d_critical_long <- d_critical_clean %>% 
  pivot_longer(c(competence, relevance, prior), 
               names_to = "class_condition", 
               values_to = "prior_class")


d_critical_long <- d_critical_long %>% 
  mutate(block = ifelse(block == "comp", "competence", 
                        ifelse(block == "rel", "relevance", ifelse(block == "pri", "prior", block) )))
```

Create more extensive condition labels.

``` {r}
# extending 'conditions' labels to include whether the utterance was present or not
d_critical_long <- d_critical_long %>% 
  mutate(
    block_extended =  ifelse(block %in% c("some", "xor"), "target", block)
  )
```

Z-scoring:
```{r}
d_critical_zScored <- d_critical_long %>% group_by(submission_id, block_extended) %>%
  mutate(block_mean = mean(response),
         block_sd = sd(response),
         response_centered = (response - block_mean)/block_sd,
         # catch the cases where sd is 0 
         response_centered = ifelse(is.na(response_centered), 0, response_centered))
        # response_centered = ifelse(is.na(response_centered), (response-block_mean), response_centered)
d_critical_zScored_wide <- d_critical_zScored %>% 
  select(submission_id, title, main_type, block_extended, response_centered) %>% 
  unique() %>% 
  pivot_wider(
    names_from = block_extended, 
    values_from = response_centered, 
    values_fn = mean # getting means for double prior measurement in "xor"
  ) 


# assume that the mistake in the z-scoring is the absence of dropping the duplicates
# or is it the point where the 0 SDs are caught?
d_critical_zScored_noDuplicates <- d_critical_long %>% 
  filter((block_extended == class_condition) | (class_condition =="relevance" & block_extended == "target") ) 

d_critical_noDuplicates_zScored <- d_critical_zScored_noDuplicates %>% group_by(submission_id, block_extended) %>%
  mutate(block_mean = mean(response),
         block_sd = sd(response),
         response_centered = (response - block_mean)/block_sd,
         # catch the cases where sd is 0 
         response_centered = ifelse(is.na(response_centered), 0, response_centered))

# perform the conceptually cleaner procedure: average over prior ratings first, then z-score
d_critical_zScored_noDuplicates %>% unique() %>% 
  select(submission_id, title, main_type, block_extended, response) %>%
  pivot_wider(
    names_from = block_extended, 
    values_from = response, 
    values_fn = mean # getting means for double prior measurement in "xor"
  ) %>%
  pivot_longer(
    cols=c(relevance, competence, prior, target),
    names_to = "block_extended",
    values_to = "response"
  ) -> d_critical_zScored_meanFirst_noDupl

d_critical_zScored_meanFirst_noDupl_zScored <- d_critical_zScored_meanFirst_noDupl %>%  
  group_by(submission_id, block_extended) %>%
  mutate(block_mean = mean(response),
         block_sd = sd(response),
         response_centered = (response - block_mean)/block_sd,
         # catch the cases where sd is 0 
         response_centered = ifelse(is.na(response_centered), 0, response_centered))

d_critical_zScored_meanFirst_noDupl_zScored_wide <- d_critical_zScored_meanFirst_noDupl_zScored %>% 
  select(submission_id, title, main_type, block_extended, response_centered) %>% 
  unique() %>% 
  pivot_wider(
    names_from = block_extended, 
    values_from = response_centered, 
    values_fn = mean # getting means for double prior measurement in "xor"
  ) 

d_critical_noDuplicates_zScored_wide <- d_critical_noDuplicates_zScored %>% 
  select(submission_id, title, main_type, block_extended, response_centered) %>% 
  unique() %>% 
  pivot_wider(
    names_from = block_extended, 
    values_from = response_centered, 
    values_fn = mean # getting means for double prior measurement in "xor"
  ) 
```
```{r}
# look at raw ratings
# df to be used before applying z-scoring in cell above 
d_critical_zScored_noDuplicates %>%
  filter(block_extended != "target") %>%
  ungroup() %>%
  group_by(block, prior_class, main_type) %>%
  summarize(mean_rating = mean(response)) -> by_type_factor_means

d_critical_zScored_noDuplicates %>%
  filter(block_extended!="target") %>%
  ggplot(., aes(x=main_type, y = response)) +
  geom_jitter(aes(color=prior_class)) +
  geom_point(data = by_type_factor_means, aes(x=main_type, y = mean_rating), color="red") +
  facet_grid(prior_class~block)
```

Z-score data when excluding participants who failed the implicature ratings: 
```{r}
d_critical_clean_noImplFails <- d_clean_exclude_implicatureFails %>% filter(condition == "critical")
d_critical_long_noImplFails <- d_critical_clean_noImplFails %>% 
  pivot_longer(c(competence, relevance, prior), 
               names_to = "class_condition", 
               values_to = "prior_class")


d_critical_long_noImplFails <- d_critical_long_noImplFails %>% 
  mutate(block = ifelse(block == "comp", "competence", 
                        ifelse(block == "rel", "relevance", ifelse(block == "pri", "prior", block) )))

d_critical_long_noImplFails <- d_critical_long_noImplFails %>% 
  mutate(
    block_extended =  ifelse(block %in% c("some", "xor"), "target", block)
  )

# z-scoring, excluding duplicates
d_critical_zScored_noImplFails_noDuplicates <- d_critical_long_noImplFails %>% 
  filter((block_extended == class_condition) | (class_condition =="relevance" & block_extended == "target") ) 

d_critical_noImplFails_noDuplicates_zScored <- d_critical_zScored_noImplFails_noDuplicates %>% group_by(submission_id, block_extended) %>%
  mutate(block_mean = mean(response),
         block_sd = sd(response),
         response_centered = (response - block_mean)/block_sd,
         # catch the cases where sd is 0 
         response_centered = ifelse(is.na(response_centered), 0, response_centered))

d_critical_noImplFails_noDuplicates_zScored_wide <- d_critical_noImplFails_noDuplicates_zScored %>% 
  select(submission_id, title, main_type, block_extended, response_centered) %>% 
  unique() %>% 
  pivot_wider(
    names_from = block_extended, 
    values_from = response_centered, 
    values_fn = mean # getting means for double prior measurement in "xor"
  ) 
```


```{r}
# create labels for first vs second prior question, order-based 
d_critical_zScored %>% 
  filter(block != "xor" & block != "some") %>%
  filter(block == class_condition) %>%
  filter(block == "prior", main_type == "xor") %>%
  ungroup() %>%
  mutate(priorQ_nr = rep(c(1,2), times=nrow(.)/2),
         priorQ_factor = paste("prior", priorQ_nr, sep = "_"),
         unique_ID = paste(submission_id, ID, sep="_")) -> d_xor_priors

# create labels for the two questions
d_xor_priors %>% group_by(ID, prompt) %>%
  select(ID, prompt) %>% distinct() %>%
  ungroup() %>%
  mutate(
    priorQ_label = rep(c("priorQ_1", "priorQ_2"), 32)
  ) -> d_xor_priors_labels

d_xor_priors_labels_full <- left_join(d_xor_priors, d_xor_priors_labels, by = c("prompt", "ID")) %>% 
  select("prompt", "ID", "block_extended", "class_condition", "submission_id", "priorQ_factor", "priorQ_label")

# create a df with both prior ratings for exploratory analysis
d_critical_zScored_xor <- d_critical_zScored %>%
  ungroup() %>%
  filter(main_type == "xor") %>%
  select(submission_id, title, main_type, block_extended, response_centered, prompt, ID, class_condition) %>% 
  unique() %>% 
  left_join(., d_xor_priors_labels_full, 
            by = c("prompt", "ID", "block_extended", "class_condition", "submission_id")
            ) %>%
  filter(block_extended == class_condition | block_extended == "target") %>%
  mutate(
    block_xor_order = ifelse("prior" == block_extended, priorQ_factor, block_extended),
    block_xor_type = ifelse("prior" == block_extended, priorQ_label, block_extended),
  ) %>% select(-priorQ_factor, -priorQ_label)

# create wide version for the order based question distinction
d_critical_zScored_wide_xor_prior_order <- d_critical_zScored_xor %>% 
  select(-block_extended, -block_xor_type, -class_condition, -prompt) %>%
  unique() %>%
  pivot_wider(
    names_from = block_xor_order, 
    values_from = response_centered
  ) 

# create wide version for the type based question distinction
d_critical_zScored_wide_xor_prior_type <- d_critical_zScored_xor %>% 
  select(-block_extended, -block_xor_order, -class_condition, -prompt) %>%
  unique() %>%
  pivot_wider(
    names_from = block_xor_type, 
    values_from = response_centered
  ) 
```

Write out z-scored data:
```{r}
d_critical_zScored %>% write_csv("./../../data/main/results_prereg_tidy_final_zScored_long.csv")
d_critical_zScored_wide %>% write_csv("./../../data/main/results_prereg_tidy_final_zScored_wide.csv")
d_critical_noDuplicates_zScored %>% write_csv("./../../data/main/results_prereg_tidy_noDuplicates_zScored_long.csv")
d_critical_noDuplicates_zScored_wide %>% write_csv("./../../data/main/results_prereg_tidy_noDuplicates_zScored_wide.csv")
d_critical_noImplFails_noDuplicates_zScored %>%
  write_csv("./../../data/main/results_prereg_tidy_noDuplicates_noImplFails_zScored_long.csv")
d_critical_noImplFails_noDuplicates_zScored_wide %>%
  write_csv("./../../data/main/results_prereg_tidy_noDuplicates_noImplFails_zScored_wide.csv")

# write out dataframes wth xor priors
d_critical_zScored_wide_xor_prior_order %>% write_csv("./../../data/main/results_prereg_tidy_final_zScored_wide_xor_priors-order.csv")

d_critical_zScored_wide_xor_prior_type %>% write_csv("./../../data/main/results_prereg_tidy_final_zScored_wide_xor_priors-type.csv")

d_critical_zScored_meanFirst_noDupl_zScored %>%
  write_csv("./../../data/main/results_prereg_tidy_noDuplicates_zScored_meansFirst_long.csv")
d_critical_zScored_meanFirst_noDupl_zScored_wide %>%
  write_csv("./../../data/main/results_prereg_tidy_noDuplicates_zScored_meansFirst_wide.csv")
```
