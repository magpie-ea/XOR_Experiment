---
title: "Xor-Some Preregistered Study: Analyses"
author: "Polina Tsvilodub"
date: "8/31/2021"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(brms)
library(tidyboot)
library(tidybayes)
```

Read data created in preprocessing script:
```{r, message=FALSE, warning=FALSE, results='hide'}
d_critical_zScored_wide <- read_csv("./../../data/main/results_prereg_tidy_final_zScored_wide.csv")
d_critical_zScored <- read_csv("./../../data/main/results_prereg_tidy_final_zScored_long.csv")

d_critical_zScored_wide_xor_prior_order <- read_csv("./../../data/main/results_prereg_tidy_final_zScored_wide_xor_priors-order.csv")

d_critical_zScored_wide_xor_prior_type <- read_csv("./../../data/main/results_prereg_tidy_final_zScored_wide_xor_priors-type.csv")
```


## Main analysis 

Next, fit the maximal model *across* experiments (i.e., *across SI trigger types*). For convergence reasons, correlation of random effects has to be set to 0. 
```{r, results='hide', message=FALSE, warning=FALSE}
# Full model 
d_critical_zScored_wide <- d_critical_zScored_wide %>% mutate(
  main_type = as.factor(main_type)
)
# some = 0, xor = 1
contrasts(d_critical_zScored_wide$main_type) 

# set priors
priors <- set_prior("student_t(1, 0, 2)", class = "b")

model_SI <- brm(target ~ prior*competence*relevance* main_type +
                   (1 + prior + competence + relevance + main_type || submission_id) +
                   (1 | title),
                 data = d_critical_zScored_wide,
                 prior = priors,
                 sample_prior = T,
                 control = list(adapt_delta = 0.95),
                 cores = 4,
                 iter = 3000)
```

```{r}
summary(model_SI)
```

Next, extract posterior contrasts of interest: effects of each predictor \{prior, competence and relevance\} for each trigger.

```{r}
model_SI %>% spread_draws(b_Intercept, b_prior, b_competence, b_relevance,
                          b_main_typexor, `b_prior:competence`,
                          `b_prior:relevance`, `b_competence:relevance`,
                          `b_prior:main_typexor`, `b_competence:main_typexor`,
                          `b_relevance:main_typexor`, `b_prior:competence:relevance`, 
                          `b_prior:competence:main_typexor`, `b_prior:relevance:main_typexor`,
                          `b_competence:relevance:main_typexor`, 
                          `b_prior:competence:relevance:main_typexor`) %>% 
  mutate(
    prior_xor = b_prior + `b_prior:main_typexor`,
    prior_some = b_prior,
    competence_xor = b_competence + `b_competence:main_typexor`,
    competence_some = b_competence,
    relevance_xor =  b_relevance + `b_relevance:main_typexor`,
    relevance_some = b_relevance
  ) -> model_SI_posteriors

# check P that the effects are positive / negative / no effect present
posterior_hypotheses <- model_SI_posteriors %>% 
  select(prior_xor, prior_some, 
         competence_xor, competence_some,
         relevance_xor, relevance_some) %>%
  gather(key, val) %>%
  group_by(key) %>% mutate(positive = mean(val > 0.05),
                           negative = mean(val < -0.05),
                           no = mean(val %>% between(-0.05, 0.05))) %>%
  summarise(positive_eff = mean(positive),
            negative_eff = mean(negative),
            no_eff = mean(no))
posterior_hypotheses
```

For the paper: Investigate whether we get the same results (qualitatively) if we set the ROPE to 0:
```{r}
posterior_hypotheses_nullROPE <- model_SI_posteriors %>% 
  select(prior_xor, prior_some, 
         competence_xor, competence_some,
         relevance_xor, relevance_some) %>%
  gather(key, val) %>%
  group_by(key) %>% mutate(positive = mean(val > 0),
                           negative = mean(val < 0)) %>% # no = mean(val = 0)
  summarise(positive_eff = mean(positive),
            negative_eff = mean(negative) # no_eff = mean(no)
            )
posterior_hypotheses_nullROPE
```

#### Perform BF analysis

We want to address a conjunctive hypothesis, one triplet for each trigger word, namely:

1. the slope of 'competence' is positive
2. the slope of 'prior' is negative
3. the slope of 'relevance' is positive

We judge there to be strong evidence for positivity of a slope variable $\beta_X$, if the posterior probability $P(\beta_X > \delta \mid D)$ is at least $.95$, for $\delta = 0.05$ the parameter that defines our 'region of practical equivalence'.
A posterior odds ratio of at least $\frac{.95}{.05} = 19$ corresponds to a Bayes factor of at least 19 when prior odds are 1.

The binary test of conformity with the theoretical predictions therefore is:
```{r}
test_conjunction_of_all_hypotheses <-  function(posterior_hypotheses) {
  posterior_hypotheses %>% 
    mutate(hypothesis_true = case_when(
      key == 'competence_some' ~ positive_eff > 0.95,
      key == 'competence_xor'  ~ positive_eff > 0.95,
      key == 'prior_some' ~ negative_eff > 0.95,
      key == 'prior_xor'  ~ negative_eff > 0.95, 
      key == 'relevance_some' ~ positive_eff > 0.95,
      key == 'relevance_xor'  ~ positive_eff > 0.95
    )) %>% 
    pull(hypothesis_true) %>% all()
}
# applied to the pilot data
test_conjunction_of_all_hypotheses(posterior_hypotheses)
```
### Plot posterior distributions

```{r, fig.height = 6, fig.width=9}
model_SI_posteriors %>% select(prior_xor, prior_some,
                               competence_xor, competence_some,
                               relevance_xor, relevance_some) %>%
  pivot_longer(cols = everything(), names_to = "effect", values_to = "value") -> model_SI_posteriors_long


model_SI_posteriors_long %>%
  mutate(effect = factor(effect, 
                         levels = c("relevance_xor", "relevance_some", "prior_xor", "prior_some", "competence_xor", "competence_some"), 
                         labels = c("Relevance (or)", "Relevance (some)", "Prior (or)", "Prior (some)", "Competence (or)", "Competence (some)"))) %>%
  ggplot(.) + # 
  stat_halfeye(aes(y = effect, x = value, 
                   fill = stat(case_when(x %>% between(-0.05, 0.05) ~ "no effect", x < -0.05 ~ "negative effect", x > 0.05 ~ "positive effect"))
                   ), 
               alpha = .7) +
  geom_vline(xintercept = c(-.05, .05), linetype = "dashed") +
  scale_fill_manual(name = "Direction of effect", values = c("coral", "yellow3", "mediumseagreen")) +
  labs(x = "Effect size", y = "Effect") +
  theme(legend.position = "top",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12),
        axis.title = element_text(size = 13),
        strip.text.x = element_text(size = 13),
        strip.text.y = element_text(size = 13))

# ggsave("../../writing/ELM2/posterior-effects.pdf", width=7, height=5)
```

## Exploratory analyses

Model with binary predictors:
```{r, results='hide', message=FALSE, warning=FALSE}
d_critical_zScored %>% 
  select(submission_id, title, main_type, block_extended, response_centered, prior_class, class_condition) %>% 
  unique() %>% 
  pivot_wider(
    names_from = class_condition, 
    values_from = prior_class
  )  %>%
  mutate(relevance = factor(relevance, levels = c(1, 0)),
         competence = factor(competence, levels = c(1, 0)),
         prior = factor(prior, levels = c(1, 0)),
         main_type = as.factor(main_type)) %>% 
  filter(block_extended == "target") -> d_critical_wide_cat_zScored
# sum code predictors
# low prior: -1, high prior:: 1 
contrasts(d_critical_wide_cat_zScored$prior) <- contr.sum(2)
# low comp : -1, high comp : 1
contrasts(d_critical_wide_cat_zScored$competence) <- contr.sum(2)
# low rel: -1, high rel: 1
contrasts(d_critical_wide_cat_zScored$relevance) <- contr.sum(2)
# leave main type dummy coded
contrasts(d_critical_wide_cat_zScored$main_type) 

# xor, maximal model with interactions and maximal REs
model_xor_cat_zScored <- brm(
  response_centered ~ prior*competence*relevance*main_type + 
    (1 + prior + competence + relevance + main_type || submission_id) +
    (1 | title),
  data = d_critical_wide_cat_zScored,
  prior = priors,
  sample_prior = T,
  family = "gaussian",
  cores = 4,
  iter = 3000
)

```
```{r}
summary(model_xor_cat_zScored)
```
Now extract posterior contrasts of interest from categorical model: effects of each predictor \{prior, competence and relevance\} for each trigger.

```{r}
model_xor_cat_zScored %>% spread_draws(b_Intercept, b_prior1, b_competence1, b_relevance1,
                          b_main_typexor, `b_prior1:competence1`,
                          `b_prior1:relevance1`, `b_competence1:relevance1`,
                          `b_prior1:main_typexor`, `b_competence1:main_typexor`,
                          `b_relevance1:main_typexor`, `b_prior1:competence1:relevance1`, 
                          `b_prior1:competence1:main_typexor`, `b_prior1:relevance1:main_typexor`,
                          `b_competence1:relevance1:main_typexor`, 
                          `b_prior1:competence1:relevance1:main_typexor`) %>% 
  mutate(
    prior_xor = 2 * b_prior1 + 2* `b_prior1:main_typexor`,
    prior_some = 2 * b_prior1,
    competence_xor = 2 * b_competence1 + 2* `b_competence1:main_typexor`,
    competence_some = 2 * b_competence1,
    relevance_xor =  2 * b_relevance1 + 2* `b_relevance1:main_typexor`,
    relevance_some = 2* b_relevance1
  ) -> model_xor_cat_zScored_posteriors

# check P that the effects are positive / negative / no effect present
posterior_hypotheses_cat <- model_xor_cat_zScored_posteriors %>% 
  select(prior_xor, prior_some, 
         competence_xor, competence_some,
         relevance_xor, relevance_some) %>%
  gather(key, val) %>%
  group_by(key) %>% mutate(positive = mean(val > 0.05),
                           negative = mean(val < -0.05),
                           no = mean(val %>% between(-0.05, 0.05))) %>%
  summarise(positive_eff = mean(positive),
            negative_eff = mean(negative),
            no_eff = mean(no))
posterior_hypotheses_cat
```

```{r}
test_conjunction_of_all_hypotheses(posterior_hypotheses_cat)
```

```{r, fig.height = 6, fig.width=9}
model_xor_cat_zScored_posteriors %>% select(prior_xor, prior_some,
                               competence_xor, competence_some,
                               relevance_xor, relevance_some) %>%
  pivot_longer(cols = everything(), names_to = "effect", values_to = "value") -> model_xor_cat_posteriors_long


model_xor_cat_posteriors_long %>%
  mutate(effect = factor(effect, 
                         levels = c("relevance_xor", "relevance_some", "prior_xor", "prior_some", "competence_xor", "competence_some"), 
                         labels = c("Relevance (or)", "Relevance (some)", "Prior (or)", "Prior (some)", "Competence (or)", "Competence (some)"))) %>%
  ggplot(.) + # 
  stat_halfeye(aes(y = effect, x = value, 
                   fill = stat(case_when(x %>% between(-0.05, 0.05) ~ "no effect", x < -0.05 ~ "negative effect", x > 0.05 ~ "positive effect"))
                   ), 
               alpha = .7) +
  geom_vline(xintercept = c(-.05, .05), linetype = "dashed") +
  scale_fill_manual(name = "Direction of effect", values = c("coral", "yellow3", "mediumseagreen")) +
  labs(x = "Effect size", y = "Effect") +
  theme(legend.position = "top",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12),
        axis.title = element_text(size = 13),
        strip.text.x = element_text(size = 13),
        strip.text.y = element_text(size = 13))

#ggsave("../../writing/paper/images/posterior-effects-categorical.pdf", width=7, height=5)
```



Model with aggregated by-story ratings:
```{r, results='hide', message=FALSE, warning=FALSE}
d_critical_zScored_wide_byItem <- d_critical_zScored_wide %>% ungroup() %>%
  group_by(title, main_type) %>%
  summarize(mean_response = mean(target),
            mean_prior = mean(prior),
            mean_relevance = mean(relevance),
            mean_competence = mean(competence))


model_means <- brm(mean_response ~ mean_prior * mean_relevance * mean_competence * main_type,
                       data = d_critical_zScored_wide_byItem,
                       prior = priors,
                       sample_prior = T,
                       family = "gaussian",
                       cores = 4,
                       iter = 3000
                       )
```

```{r}
summary(model_means)
```

Extract posterior contrasts of interest from by-story mean based model: effects of each predictor \{prior, competence and relevance\} for each trigger.

```{r}
model_means %>% spread_draws(b_Intercept, b_mean_prior, b_mean_competence, b_mean_relevance,
                          b_main_typexor, `b_mean_prior:mean_competence`,
                          `b_mean_prior:mean_relevance`, `b_mean_relevance:mean_competence`,
                          `b_mean_prior:main_typexor`, `b_mean_competence:main_typexor`,
                          `b_mean_relevance:main_typexor`,
                          `b_mean_prior:mean_relevance:mean_competence`, 
                          `b_mean_prior:mean_competence:main_typexor`,
                          `b_mean_prior:mean_relevance:main_typexor`,
                          `b_mean_relevance:mean_competence:main_typexor`, 
                          `b_mean_prior:mean_relevance:mean_competence:main_typexor`) %>% 
  mutate(
    prior_xor = b_mean_prior + `b_mean_prior:main_typexor`,
    prior_some = b_mean_prior,
    competence_xor = b_mean_competence + `b_mean_competence:main_typexor`,
    competence_some = b_mean_competence,
    relevance_xor =  b_mean_relevance + `b_mean_relevance:main_typexor`,
    relevance_some = b_mean_relevance
  ) -> model_means_posteriors

# check P that the effects are positive / negative / no effect present
posterior_hypotheses_means <- model_means_posteriors %>% 
  select(prior_xor, prior_some, 
         competence_xor, competence_some,
         relevance_xor, relevance_some) %>%
  gather(key, val) %>%
  group_by(key) %>% mutate(positive = mean(val > 0.05),
                           negative = mean(val < -0.05),
                           no = mean(val %>% between(-0.05, 0.05))) %>%
  summarise(positive_eff = mean(positive),
            negative_eff = mean(negative),
            no_eff = mean(no))
posterior_hypotheses_means
```

```{r}
test_conjunction_of_all_hypotheses(posterior_hypotheses_means)
```

```{r, fig.height = 6, fig.width=9}
model_means_posteriors %>% select(prior_xor, prior_some,
                               competence_xor, competence_some,
                               relevance_xor, relevance_some) %>%
  pivot_longer(cols = everything(), names_to = "effect", values_to = "value") -> model_mean_posteriors_long


model_mean_posteriors_long %>%
  mutate(effect = factor(effect, 
                         levels = c("relevance_xor", "relevance_some", "prior_xor", "prior_some", "competence_xor", "competence_some"), 
                         labels = c("Relevance (or)", "Relevance (some)", "Prior (or)", "Prior (some)", "Competence (or)", "Competence (some)"))) %>%
  ggplot(.) + # 
  stat_halfeye(aes(y = effect, x = value, 
                   fill = stat(case_when(x %>% between(-0.05, 0.05) ~ "no effect", x < -0.05 ~ "negative effect", x > 0.05 ~ "positive effect"))
                   ), 
               alpha = .7) +
  geom_vline(xintercept = c(-.05, .05), linetype = "dashed") +
  scale_fill_manual(name = "Direction of effect", values = c("coral", "yellow3", "mediumseagreen")) +
  labs(x = "Effect size", y = "Effect") +
  theme(legend.position = "top",
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12),
        axis.title = element_text(size = 13),
        strip.text.x = element_text(size = 13),
        strip.text.y = element_text(size = 13))
#ggsave("../../writing/paper/images/posterior-effects-means.pdf", width=7, height=5)
```

A non-preregistered, but nevertheless possibly interesting exploratory analysis including the two individual prior ratings of the conditionals in the xor condition as predictors (as opposed to the mean of the two used in the preceding models):
Analysis based on the order of appearance of the questions:
```{r, results='hide', message=FALSE, warning=FALSE}

# set priors
priors <- set_prior("student_t(1, 0, 2)", class = "b")

model_xor_priors_order <- brm(target ~ prior_1*prior_2*competence*relevance +
                   (1 + prior_1 + prior_2 + competence + relevance || submission_id) +
                   (1 | title),
                 data = d_critical_zScored_wide_xor_prior_order,
                 prior = priors,
                 sample_prior = T,
                 control = list(adapt_delta = 0.95),
                 cores = 4,
                 iter = 3000)
```

```{r}
summary(model_xor_priors_order)
```

Check the intervals of intervals, computing them separately using only one prior estimate, to see if we get different results:
```{r}
model_xor_priors_order %>% spread_draws(b_Intercept, b_prior_1, b_prior_2, b_competence, b_relevance) %>% 
  mutate(
    prior1_xor = b_prior_1,
    prior2_xor = b_prior_2,
    competence_xor = b_competence,
    relevance_xor =  b_relevance
  ) -> model_xor_priors_posteriors

# check P that the effects are positive / negative / no effect present
posterior_hypotheses_xor_priors_order <- model_xor_priors_posteriors %>% 
  select(prior1_xor, prior2_xor, 
         competence_xor, relevance_xor) %>%
  gather(key, val) %>%
  group_by(key) %>% mutate(positive = mean(val > 0.05),
                           negative = mean(val < -0.05),
                           no = mean(val %>% between(-0.05, 0.05))) %>%
  summarise(positive_eff = mean(positive),
            negative_eff = mean(negative),
            no_eff = mean(no))
posterior_hypotheses_xor_priors_order
```

Prior question type based analysis:
```{r, results='hide', message=FALSE, warning=FALSE}

# set priors
priors <- set_prior("student_t(1, 0, 2)", class = "b")

model_xor_priors_type <- brm(target ~ priorQ_1*priorQ_2*competence*relevance +
                   (1 + priorQ_1 + priorQ_2 + competence + relevance || submission_id) +
                   (1 | title),
                 data = d_critical_zScored_wide_xor_prior_type,
                 prior = priors,
                 sample_prior = T,
                 control = list(adapt_delta = 0.95),
                 cores = 4,
                 iter = 3000)
```

```{r}
summary(model_xor_priors_type)
```

Check the intervals of intervals, computing them separately using only one prior estimate, to see if we get different results:
```{r}
model_xor_priors_type %>% spread_draws(b_Intercept, b_priorQ_1, b_priorQ_2, b_competence, b_relevance) %>% 
  mutate(
    priorQ1_xor = b_priorQ_1,
    priorQ2_xor = b_priorQ_2,
    competence_xor = b_competence,
    relevance_xor =  b_relevance
  ) -> model_xor_priors_posteriors

# check P that the effects are positive / negative / no effect present
posterior_hypotheses_xor_priors_type <- model_xor_priors_posteriors %>% 
  select(priorQ1_xor, priorQ2_xor, 
         competence_xor, relevance_xor) %>%
  gather(key, val) %>%
  group_by(key) %>% mutate(positive = mean(val > 0.05),
                           negative = mean(val < -0.05),
                           no = mean(val %>% between(-0.05, 0.05))) %>%
  summarise(positive_eff = mean(positive),
            negative_eff = mean(negative),
            no_eff = mean(no))
posterior_hypotheses_xor_priors_type
```